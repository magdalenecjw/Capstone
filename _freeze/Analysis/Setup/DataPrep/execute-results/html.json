{
  "hash": "eceec5b88637d81f344525707c707df7",
  "result": {
    "markdown": "---\npagetitle: Data Preparation | Setting Up | Capstone\ntitle: Data Preparation\nauthor: \"Magdalene Chan\"\ndate: 2024-03-09\nexecute: \n  warning: false\nformat:\n  html:\n    code-fold: true\n---\n\n\n![](../../Images/2.png){width=\"500\"}\n\n# Getting Started\n\nThe code chunk below uses `p_load()` function of pacman package to check if the required packages have been installed on the computer. If they are, the packages will be launched.\n\nThe packages used are:\n\n-   arrow package to read and write Parquet files, which is the format that our source data is in.\n-   fs package for smoother file handling.\n-   tidyverse package for aspatial data wrangling.\n-   lubridate package to handle date and time data.\n-   data.table package for fast aggregation of large datasets.\n-   sf package is used for importing, managing, and processing geospatial data.\n-   stplanr package is used to create and plot desire lines on maps.\n-   tmap package is used to visualize geospatial data on maps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(arrow, fs, tidyverse, lubridate, data.table, sf, stplanr, tmap)\n```\n:::\n\n\n# Preparing base map\n\nThe basemap is created using the URA Master Plan 2019, which includes sub-zone boundaries in Singapore. The code chunk below uses `st_read()` function of sf package to import MPSZ-2019 shapefile into R as a polygon feature data frame. As this set of geospatial data is in WGS-84 coordinate system, `st_transform()` function is used to convert it to the svy21 projected coordinate system (`crs = 3414`) that Singapore uses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz = st_read(dsn = \"../data/geospatial\", \n               layer = \"MPSZ-2019\") %>%\n  st_transform(crs = 3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `MPSZ-2019' from data source \n  `C:\\magdalenecjw\\Capstone\\Analysis\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n```\n:::\n:::\n\n\n`qtm()` function of tmap package is then used to plot a simple choropleth map to visualise the subzone boundaries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\nqtm(mpsz)\n```\n\n::: {.cell-output-display}\n![](DataPrep_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nGiven that this geospatial dataset only has subzone boundaries without road network lines, there is a need to integrate the latter into the dataset to create a more comprehensive base map. The code chunk below uses `st_read()` function of sf package to import the OpenStreetMap \\> Asia \\> Malaysia, Singapore and Brunei shapefile extract from the [GeoFabrik website](https://download.geofabrik.de/). As this set of geospatial data is in WGS-84 coordinate system, `st_transform()` function is used to convert it to the svy21 projected coordinate system (`crs = 3414`) that Singapore uses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroads = st_read(dsn = \"../data/geospatial\", \n               layer = \"gis_osm_roads_free_1\") %>%\n  st_transform(crs = 3414)\n\nmpsz_road <- mpsz %>%\n  st_intersection(roads)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n`tm_shape()` and `tm_lines()` functions of tmap package are then used to plot a simple choropleth map to visualise the roads within various planning subzones.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_road %>%\n           filter(PLN_AREA_N == \"ORCHARD\")) + \n  tm_lines()\n```\n\n::: {.cell-output-display}\n![](DataPrep_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n#Loading Grab dataset\n\nThe dataset is provided by Grab over 10 separate files (in Apache Parquet file format), with each trajectory ID (primary identifier of each trip) found in multiple files. There is a need to join the 10 files before performing any further data wrangling. In the code chunk below, `read_parquet()` from arrow package is used to read the Parquet files. As there are 10 such files, a loop function is used to load each Parquet file within the file directory and stored into a list, which is finally then merged using `bind_rows()` function from dplyr package. Lastly, as the `pingtimestamp` field is in integer format, it is converted into date-time format using `as_datetime()` function of lubridate package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# List Parquet files in the directory\nparquet_files <- dir_ls(\"../data/\", regexp = \"\\\\.parquet$\")\n\n# Initialize an empty list to store dataframes\ndfs <- list()\n\n# Loop through each Parquet file, read it, and store in the list\nfor (file_path in parquet_files) {\n  df <- read_parquet(file_path)\n  dfs <- append(dfs, list(df))\n}\n\n# Combine individual dataframes into one dataframe\ngrab <- bind_rows(dfs) %>%\n  # Convert pingtimestamp column to date-time format\n  mutate(pingtimestamp = as_datetime(pingtimestamp))\n\n# Remove objects that will no longer be used\nrm(df, dfs, file_path, parquet_files)\n```\n:::\n\n\nFor sorting a large tibble data frame efficiently in R, while dplyr package offers the `arrange()` function, it typically does not perform the task as quickly as `setorder()` function of data.table package. data.table package often offers better performance for sorting large datasets due to its efficient memory management and optimized algorithms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set tibble dataframe as data.table \n# Then sort by trajectory ID and pingtimestamp\ngrab_sorted <- setorder(as.data.table(grab), trj_id, pingtimestamp)\n\n# Convert back to tibble dataframe\ngrab_sorted <- as_tibble(grab_sorted)\n\n# Remove objects that will no longer be used\nrm(grab)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Data Wrangling\n\nThere is a need to identify the Pick-up locations. In the code chunk below, `group_by()` function of dplyr package is used to group the data by trajectory ID, before taking the GPS coordinates (latitude and longitude) of the earliest GPS ping timestamp using `filter()` and `min()` function.\n\nSimilar steps are then performed to identify the Drop-off locations. `group_by()` function of dplyr package is used to group the data by trajectory ID, before taking the GPS coordinates (latitude and longitude) of the latest GPS ping timestamp using `filter()` and `max()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify Pick-up locations\npickup_loc <- grab_sorted %>%\n  group_by(trj_id) %>%\n  filter(pingtimestamp == min(pingtimestamp)) %>%\n  ungroup() %>%\n  select(trj_id, rawlng, rawlat) %>%\n  rename(pickup_lng = rawlng, pickup_lat = rawlat)\n\n# Identify Drop-off locations\ndropoff_loc <- grab_sorted %>%\n  group_by(trj_id) %>%\n  filter(pingtimestamp == max(pingtimestamp)) %>%\n  ungroup() %>%\n  select(trj_id, rawlng, rawlat) %>%\n  rename(dropoff_lng = rawlng, dropoff_lat = rawlat)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n# Converting aspatial data to geospatial data\n\nTo convert the aspatial data to geospatial data, the latitude (`rawlat`) and longitude (`rawlng`) columns will be used. These columns are in decimal degree format, indicating that the data is in wgs84 geographic coordinate system. The code chunk below converts the `grab_sorted` object into a sf data frame using `st_as_sf()` function of sf package then `st_transform()` function to convert it into the svy21 projected coordinate system that Singapore uses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npickup_sf <- st_as_sf(pickup_loc,\n                      coords = c(\"pickup_lng\", \"pickup_lat\"),\n                      crs = 4326) %>%\n  st_transform(crs = 3414)\n\ndropoff_sf <- st_as_sf(dropoff_loc,\n                       coords = c(\"dropoff_lng\", \"dropoff_lat\"),\n                       crs = 4326) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n\nTo ensure the transformation has been done correctly, the newly created sf data frames will be visualised on a choropleth map using tmap package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualise drop-off locations\ntm_shape(mpsz) + \n  tm_polygons() +\ntm_shape(pickup_sf) + \n  tm_dots()\n```\n\n::: {.cell-output-display}\n![](DataPrep_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualise drop-off locations\ntm_shape(mpsz) + \n  tm_polygons() +\ntm_shape(dropoff_sf) + \n  tm_dots()\n```\n\n::: {.cell-output-display}\n![](DataPrep_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n# Creating O-D flow matrix\n\nApart from pick-up and drop-off point patterns, another objective of this study is to study the origin-destination flow patterns. As such, there is a need to create an origin-destination flow matrix where each row relates to one trajectory ID (trip) and represents travel from origin to destination.\n\n`od_coords2line()` function of stplanr package is used to convert origin-destination coordinates into desire lines. The function takes in a data frame or matrix representing the coordinates of origin-destination data, where the first two columns represent the coordinates of the origin points and the third and fourth columns represent the coordinates of the destination (in the same CRS).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodflow <- pickup_loc %>%\n  left_join(dropoff_loc, by = \"trj_id\")\n\nodflow_sf <- od_coords2line(odflow %>%\n                              select(-\"trj_id\"),\n                            crs = 4326) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "DataPrep_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}